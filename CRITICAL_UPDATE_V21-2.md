# üîß CRITICAL UPDATE V2.1 - –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∫–æ–¥–∞

**–î–∞—Ç–∞:** 21 —è–Ω–≤–∞—Ä—è 2026  
**–í–µ—Ä—Å–∏—è:** 2.1  
**–í—Ä–µ–º—è –Ω–∞ —á—Ç–µ–Ω–∏–µ:** 15 –º–∏–Ω—É—Ç  
**–î–ª—è –∫–æ–≥–æ:** Backend-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏, AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã (CLINE)

---

## üìñ –°–û–î–ï–†–ñ–ê–ù–ò–ï

1. [–ü—Ä–æ–±–ª–µ–º–∞ –∏ —Ä–µ—à–µ–Ω–∏–µ](#–ø—Ä–æ–±–ª–µ–º–∞-–∏-—Ä–µ—à–µ–Ω–∏–µ)
2. [–ú–∏–≥—Ä–∞—Ü–∏—è –Ω–∞ Google Gemini](#–º–∏–≥—Ä–∞—Ü–∏—è-–Ω–∞-google-gemini)
3. [–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏](#–∏–∑–º–µ–Ω–µ–Ω–∏–µ-—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏)
4. [–ù–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ–∏—Å–∫–∞](#–Ω–æ–≤–∞—è-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞-–ø–æ–∏—Å–∫–∞)
5. [–í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å](#–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è-—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å)
6. [–°–∏—Å—Ç–µ–º–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤](#—Å–∏—Å—Ç–µ–º–∞-—Ñ–∏–ª—å—Ç—Ä–æ–≤)
7. [–ü—Ä–∏–º–µ—Ä—ã –ø–æ–ª–Ω–æ–≥–æ –∫–æ–¥–∞](#–ø—Ä–∏–º–µ—Ä—ã-–ø–æ–ª–Ω–æ–≥–æ-–∫–æ–¥–∞)

---

## üéØ –ü–†–û–ë–õ–ï–ú–ê –ò –†–ï–®–ï–ù–ò–ï

### –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ MVP (‚ùå)

```python
# backend/embeddings.py - –¢–ï–ö–£–©–ò–ô –ö–û–î (–ù–ï–ü–†–ê–í–ò–õ–¨–ù–û)
from sentence_transformers import SentenceTransformer
import numpy as np

class EmbeddingService:
    def __init__(self):
        # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é mini-LLM
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.dimension = 384  # ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –†–ê–ó–ú–ï–†–ù–û–°–¢–¨
    
    def embed_text(self, text: str) -> np.ndarray:
        """–°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é 384"""
        return self.model.encode(text)
    
    def embed_batch(self, texts: list[str]) -> np.ndarray:
        """–°–æ–∑–¥–∞–µ—Ç batch —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ 384"""
        return self.model.encode(texts)
```

**–ü—Ä–æ–±–ª–µ–º—ã:**
1. ‚ùå –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å (sentence-transformers) –≤–º–µ—Å—Ç–æ Google Gemini
2. ‚ùå –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å - —á—Ç–æ–±—ã –≤–µ–∑–¥–µ 384 
3. ‚ùå –ù–µ—Ç –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è –ø–æ –ø–æ–ª—è–º
4. ‚ùå –°–º–µ—à–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ (—Å–µ–º–∞–Ω—Ç–∏–∫–∞ + –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)

### –¶–µ–ª–µ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ (‚úÖ)

```python
# backend/embeddings.py - –ù–û–í–´–ô –ö–û–î (–ü–†–ê–í–ò–õ–¨–ù–û)
import google.generativeai as genai
import numpy as np
import os
from typing import List

class EmbeddingService:
    def __init__(self):
        # –ü–æ–¥–∫–ª—é—á–∞–µ–º Google Gemini API
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")
        
        genai.configure(api_key=api_key)
        self.model_name = "models/text-embedding-004"
        self.dimension = 384  # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–ê–Ø –†–ê–ó–ú–ï–†–ù–û–°–¢–¨
    
    def embed_text(self, text: str, task_type: str = "retrieval_document") -> np.ndarray:
        """
        –°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é 384 —á–µ—Ä–µ–∑ Google Gemini API
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏ (retrieval_document –∏–ª–∏ retrieval_query)
        
        Returns:
            numpy array —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é 384
        """
        result = genai.embed_content(
            model=self.model_name,
            content=text,
            task_type=task_type
        )
        return np.array(result['embedding'])
    
    def embed_batch(self, texts: List[str], task_type: str = "retrieval_document") -> np.ndarray:
        """
        –°–æ–∑–¥–∞–µ—Ç batch —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é 384
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏
        
        Returns:
            numpy array —Ñ–æ—Ä–º—ã (len(texts), 384)
        """
        embeddings = []
        for text in texts:
            result = genai.embed_content(
                model=self.model_name,
                content=text,
                task_type=task_type
            )
            embeddings.append(result['embedding'])
        
        return np.array(embeddings)
```

---

## üîÑ –ú–ò–ì–†–ê–¶–ò–Ø –ù–ê GOOGLE GEMINI

### –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

**–£–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ:**
```bash
pip uninstall -y sentence-transformers transformers torch torchvision torchaudio
```

**–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–æ–≤—ã–µ:**
```bash
pip install google-generativeai python-dotenv
```

**–û–±–Ω–æ–≤–∏—Ç—å requirements.txt:**
```txt
# –°—Ç–∞—Ä–æ–µ (—É–¥–∞–ª–∏—Ç—å):
# sentence-transformers==2.2.2
# transformers==4.30.0
# torch==2.0.1

# –ù–æ–≤–æ–µ (–¥–æ–±–∞–≤–∏—Ç—å):
google-generativeai==0.3.1
python-dotenv==1.0.0
numpy==1.24.3
scikit-learn==1.3.0
```

### –®–∞–≥ 2: –ü–æ–ª—É—á–µ–Ω–∏–µ API Key

1. –ü–µ—Ä–µ–π—Ç–∏ –Ω–∞ https://aistudio.google.com
2. –ù–∞–∂–∞—Ç—å "Get API Key"
3. –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π API Key –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π
4. –°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –∫–ª—é—á

### –®–∞–≥ 3: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ .env

–°–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª `.env` –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞:
```bash
# .env
GOOGLE_API_KEY=AIzaSyDxxxxxxxxxxxxxxxxxxxxxxxxxx

# –î—Ä—É–≥–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
DATABASE_URL=postgresql://user:pass@localhost/db
DEBUG=True
```

**–î–æ–±–∞–≤–∏—Ç—å .env –≤ .gitignore:**
```bash
echo ".env" >> .gitignore
```

### –®–∞–≥ 4: –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è

```python
# backend/config.py
from dotenv import load_dotenv
import os

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ .env
load_dotenv()

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
class Config:
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
    DATABASE_URL = os.getenv("DATABASE_URL")
    EMBEDDING_DIMENSION = 384  # ‚úÖ –í–°–ï–ì–î–ê 384
    
    @classmethod
    def validate(cls):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        if not cls.GOOGLE_API_KEY:
            raise ValueError("GOOGLE_API_KEY –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ .env —Ñ–∞–π–ª–µ")
        if not cls.DATABASE_URL:
            raise ValueError("DATABASE_URL –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ .env —Ñ–∞–π–ª–µ")

# –í—ã–∑–≤–∞—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ
Config.validate()
```

---

## üìê –ò–ó–ú–ï–ù–ï–ù–ò–ï –†–ê–ó–ú–ï–†–ù–û–°–¢–ò

### –ì–¥–µ –∏–∑–º–µ–Ω–∏—Ç—å 384 ‚Üí 384

#### 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Å—Å–∏–≤–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

**–ë—ã–ª–æ:**
```python
# ‚ùå –°–¢–ê–†–´–ô –ö–û–î
embeddings = np.zeros((7283, 384))
query_embedding = np.zeros(384)
```

**–°—Ç–∞–ª–æ:**
```python
# ‚úÖ –ù–û–í–´–ô –ö–û–î
embeddings = np.zeros((7283, 384))
query_embedding = np.zeros(384)
```

#### 2. –ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞

**–ë—ã–ª–æ:**
```python
# ‚ùå –°–¢–ê–†–´–ô –ö–û–î
def load_embeddings(file_path: str) -> np.ndarray:
    """–ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ 384"""
    embeddings = np.load(file_path)
    assert embeddings.shape[1] == 384, "–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å"
    return embeddings
```

**–°—Ç–∞–ª–æ:**
```python
# ‚úÖ –ù–û–í–´–ô –ö–û–î
def load_embeddings(file_path: str) -> np.ndarray:
    """–ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ 384"""
    embeddings = np.load(file_path)
    assert embeddings.shape[1] == 384, f"–û–∂–∏–¥–∞–µ—Ç—Å—è 384, –ø–æ–ª—É—á–µ–Ω–æ {embeddings.shape[1]}"
    return embeddings
```

#### 3. –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –≤ –ë–î (PostgreSQL —Å pgvector)

**–ë—ã–ª–æ:**
```sql
-- ‚ùå –°–¢–ê–†–ê–Ø –°–•–ï–ú–ê
CREATE TABLE IF NOT EXISTS documents (
    id SERIAL PRIMARY KEY,
    content TEXT,
    embedding vector(384)  -- ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û
);

CREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops);
```

**–°—Ç–∞–ª–æ:**
```sql
-- ‚úÖ –ù–û–í–ê–Ø –°–•–ï–ú–ê
CREATE TABLE IF NOT EXISTS documents (
    id SERIAL PRIMARY KEY,
    content TEXT,
    FAS_arguments_embedding vector(384),      -- ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
    violation_summary_embedding vector(384),  -- ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
    addescription_embedding vector(384)       -- ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
);

-- –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
CREATE INDEX ON documents USING ivfflat (FAS_arguments_embedding vector_cosine_ops);
CREATE INDEX ON documents USING ivfflat (violation_summary_embedding vector_cosine_ops);
CREATE INDEX ON documents USING ivfflat (addescription_embedding vector_cosine_ops);
```

#### 4. –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

**–°–∫—Ä–∏–ø—Ç –º–∏–≥—Ä–∞—Ü–∏–∏:**
```python
# scripts/migrate_embeddings.py
import numpy as np
from backend.embeddings import EmbeddingService
from backend.database import Database
from tqdm import tqdm

def migrate_embeddings():
    """
    –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é 384
    """
    print("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –º–∏–≥—Ä–∞—Ü–∏—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    embedding_service = EmbeddingService()
    db = Database()
    
    # –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    documents = db.get_all_documents()
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    # –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    for doc in tqdm(documents, desc="–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"):
        # –¢—Ä–∏ –ø–æ–ª—è –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        fields = {
            'FAS_arguments': doc['FAS_arguments'],
            'violation_summary': doc['violation_summary'],
            'addescription': doc['addescription']
        }
        
        # –°–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—è
        embeddings = {}
        for field_name, field_text in fields.items():
            if field_text and field_text.strip():
                emb = embedding_service.embed_text(
                    field_text,
                    task_type="retrieval_document"
                )
                embeddings[f'{field_name}_embedding'] = emb
            else:
                # –ï—Å–ª–∏ –ø–æ–ª–µ –ø—É—Å—Ç–æ–µ, –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä
                embeddings[f'{field_name}_embedding'] = np.zeros(384)
        
        # –û–±–Ω–æ–≤–∏—Ç—å –≤ –ë–î
        db.update_embeddings(doc['id'], embeddings)
    
    print("‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

if __name__ == "__main__":
    migrate_embeddings()
```

---

## üîç –ù–û–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ü–û–ò–°–ö–ê

### –°—Ç–∞—Ä–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (‚ùå)

```python
# ‚ùå –°–¢–ê–†–´–ô –ü–û–î–•–û–î (–ù–ï–ü–†–ê–í–ò–õ–¨–ù–û)
def search_old(query: str, filters: dict) -> list:
    """
    –°—Ç–∞—Ä–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: —Ñ–∏–ª—å—Ç—Ä—ã –î–û –ø–æ–∏—Å–∫–∞
    """
    # 1. –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã –°–†–ê–ó–£
    filtered_docs = apply_filters(all_documents, filters)
    
    # 2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–º
    query_embedding = embedding_service.embed_text(query)
    semantic_results = vector_search(query_embedding, filtered_docs)
    
    # 3. –ï—Å–ª–∏ –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ - –¥–æ–±–∞–≤–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    if len(semantic_results) < 10:
        keyword_results = keyword_search(query, filtered_docs)
        semantic_results.extend(keyword_results)
    
    # 4. –í–µ—Ä–Ω—É—Ç—å —Ç–æ–ø-10-20
    return semantic_results[:20]
```

**–ü—Ä–æ–±–ª–µ–º—ã:**
- –§–∏–ª—å—Ç—Ä—ã —É–±–∏–≤–∞—é—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–æ –ø–æ–∏—Å–∫–∞
- –°–º–µ—à–∏–≤–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —É—Å–ª–æ–∂–Ω—è–µ—Ç –ª–æ–≥–∏–∫—É
- –ù–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (10-20)

### –ù–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (‚úÖ)

```python
# ‚úÖ –ù–û–í–´–ô –ü–û–î–•–û–î (–ü–†–ê–í–ò–õ–¨–ù–û)
def search_new(query: str, filters: dict) -> list:
    """
    –ù–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: TOP-50 ‚Üí —Ñ–∏–ª—å—Ç—Ä—ã ‚Üí –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ ‚Üí TOP-10
    """
    # 1. –°–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
    query_embedding = embedding_service.embed_text(
        query,
        task_type="retrieval_query"  # –í–∞–∂–Ω–æ: query, –Ω–µ document!
    )
    
    # 2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –í–°–ï–ú –¥–æ–∫—É–º–µ–Ω—Ç–∞–º ‚Üí TOP-50
    top_50_candidates = vector_search_top50(query_embedding)
    
    # 3. –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã –∫ TOP-50
    filtered_candidates = apply_filters(top_50_candidates, filters)
    
    # 4. –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç—Ä–µ–º –ø–æ–ª—è–º
    scored_results = calculate_weighted_scores(
        filtered_candidates,
        query_embedding
    )
    
    # 5. –í–µ—Ä–Ω—É—Ç—å —Ä–æ–≤–Ω–æ TOP-10
    return scored_results[:10]
```

### –ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

```python
# backend/search.py
from typing import List, Dict
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class SearchService:
    def __init__(self, embedding_service, database):
        self.embedding_service = embedding_service
        self.db = database
        
        # –í–µ—Å–∞ –¥–ª—è –ø–æ–ª–µ–π (–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ï–∫–∞—Ç–µ—Ä–∏–Ω—ã)
        self.field_weights = {
            'FAS_arguments': 1.0,
            'violation_summary': 0.8,
            'addescription': 0.6
        }
    
    def search(self, query: str, filters: Dict = None) -> List[Dict]:
        """
        –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            filters: –°–ª–æ–≤–∞—Ä—å —Ñ–∏–ª—å—Ç—Ä–æ–≤ {year, region, industry, article}
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∏–∑ 10 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        # –®–∞–≥ 1: –≠–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.embedding_service.embed_text(
            query,
            task_type="retrieval_query"
        )
        
        # –®–∞–≥ 2: –ü–æ–∏—Å–∫ TOP-50 –ø–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
        top_50 = self._vector_search_top50(query_embedding)
        
        # –®–∞–≥ 3: –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã
        filtered = self._apply_filters(top_50, filters or {})
        
        # –®–∞–≥ 4: –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
        scored = self._calculate_weighted_scores(filtered, query_embedding)
        
        # –®–∞–≥ 5: –í–µ—Ä–Ω—É—Ç—å TOP-10
        return scored[:10]
    
    def _vector_search_top50(self, query_embedding: np.ndarray) -> List[Dict]:
        """
        –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ TOP-50 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        
        –ü–æ–∏—Å–∫ –≤–µ–¥–µ—Ç—Å—è –ø–æ –ø–æ–ª—é FAS_arguments (—Å–∞–º—ã–π –≤–∞–∂–Ω—ã–π –≤–µ—Å 1.0)
        """
        # SQL –∑–∞–ø—Ä–æ—Å —Å pgvector
        query = """
            SELECT 
                id,
                document_url,
                document_date,
                FASdivision,
                defendant_industry,
                legal_provisions,
                FAS_arguments,
                FAS_arguments_embedding,
                violation_summary,
                violation_summary_embedding,
                addescription,
                addescription_embedding,
                (1 - (FAS_arguments_embedding <=> %s::vector)) as similarity
            FROM documents
            ORDER BY FAS_arguments_embedding <=> %s::vector
            LIMIT 50
        """
        
        # –í—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å
        results = self.db.execute(query, (query_embedding, query_embedding))
        return results
    
    def _apply_filters(self, documents: List[Dict], filters: Dict) -> List[Dict]:
        """
        –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã –∫ —Å–ø–∏—Å–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        
        –õ–æ–≥–∏–∫–∞:
        - –ú–µ–∂–¥—É —Ñ–∏–ª—å—Ç—Ä–∞–º–∏: AND (–ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ)
        - –í–Ω—É—Ç—Ä–∏ —Ñ–∏–ª—å—Ç—Ä–∞: OR (–µ—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏–π)
        """
        filtered = documents
        
        # –§–∏–ª—å—Ç—Ä –ø–æ –≥–æ–¥—É
        if filters.get('year'):
            years = filters['year'] if isinstance(filters['year'], list) else [filters['year']]
            filtered = [
                doc for doc in filtered
                if doc['document_date'].year in years
            ]
        
        # –§–∏–ª—å—Ç—Ä –ø–æ —Ä–µ–≥–∏–æ–Ω—É
        if filters.get('region'):
            regions = filters['region'] if isinstance(filters['region'], list) else [filters['region']]
            filtered = [
                doc for doc in filtered
                if doc['FASdivision'] in regions
            ]
        
        # –§–∏–ª—å—Ç—Ä –ø–æ –æ—Ç—Ä–∞—Å–ª–∏
        if filters.get('industry'):
            industries = filters['industry'] if isinstance(filters['industry'], list) else [filters['industry']]
            filtered = [
                doc for doc in filtered
                if doc['defendant_industry'] in industries
            ]
        
        # –§–∏–ª—å—Ç—Ä –ø–æ —Å—Ç–∞—Ç—å–µ (—Å–æ–¥–µ—Ä–∂–∏—Ç)
        if filters.get('article'):
            articles = filters['article'] if isinstance(filters['article'], list) else [filters['article']]
            filtered = [
                doc for doc in filtered
                if any(art in doc['legal_provisions'] for art in articles)
            ]
        
        return filtered
    
    def _calculate_weighted_scores(self, documents: List[Dict], query_embedding: np.ndarray) -> List[Dict]:
        """
        –†–∞—Å—Å—á–∏—Ç–∞—Ç—å –≤–∑–≤–µ—à–µ–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        
        –§–æ—Ä–º—É–ª–∞: S = 1.0*R_FAS + 0.8*R_violation + 0.6*R_ad
        """
        scored_docs = []
        
        for doc in documents:
            # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –ø–æ –∫–∞–∂–¥–æ–º—É –ø–æ–ª—é
            scores = {}
            for field in ['FAS_arguments', 'violation_summary', 'addescription']:
                field_embedding = doc[f'{field}_embedding']
                similarity = cosine_similarity(
                    query_embedding.reshape(1, -1),
                    field_embedding.reshape(1, -1)
                )[0][0]
                
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ [0, 1] (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
                scores[field] = max(0.0, min(1.0, similarity))
            
            # –í–∑–≤–µ—à–µ–Ω–Ω—ã–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –±–∞–ª–ª
            final_score = (
                self.field_weights['FAS_arguments'] * scores['FAS_arguments'] +
                self.field_weights['violation_summary'] * scores['violation_summary'] +
                self.field_weights['addescription'] * scores['addescription']
            )
            
            # –î–æ–±–∞–≤–∏—Ç—å –±–∞–ª–ª –≤ –¥–æ–∫—É–º–µ–Ω—Ç
            doc['final_score'] = final_score
            doc['field_scores'] = scores
            scored_docs.append(doc)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ —É–±—ã–≤–∞–Ω–∏—é –±–∞–ª–ª–∞
        scored_docs.sort(key=lambda x: x['final_score'], reverse=True)
        
        return scored_docs
```

---

## ‚öñÔ∏è –í–ó–í–ï–®–ï–ù–ù–ê–Ø –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–¨

### –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å

```
S = w_V * R_V + w_B * R_B + w_A * R_A

–≥–¥–µ:
  S - —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –±–∞–ª–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
  R_V - –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ø–æ–ª—è FAS_arguments
  R_B - –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ø–æ–ª—è violation_summary
  R_A - –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ø–æ–ª—è addescription
  w_V = 1.0 (–≤–µ—Å FAS_arguments)
  w_B = 0.8 (–≤–µ—Å violation_summary)
  w_A = 0.6 (–≤–µ—Å addescription)
```

### –ü–æ—á–µ–º—É –∏–º–µ–Ω–Ω–æ —ç—Ç–∏ –≤–µ—Å–∞?

| –ü–æ–ª–µ | –í–µ—Å | –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ |
|------|-----|-------------|
| **FAS_arguments** | 1.0 | –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ç–µ–∑–∏—Å—ã, —Ü–∏—Ç–∞—Ç—ã, –ø–æ–∑–∏—Ü–∏–∏ –§–ê–° - —Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ |
| **violation_summary** | 0.8 | –ö—Ä–∞—Ç–∫–∞—è –ª–æ–≥–∏–∫–∞ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞—Ä—É—à–µ–Ω–∏—è - –æ—á–µ–Ω—å –≤–∞–∂–Ω–æ |
| **addescription** | 0.6 | –§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ä–µ–∫–ª–∞–º—ã - –≤–∞–∂–Ω–æ, –Ω–æ –º–µ–Ω–µ–µ –∫—Ä–∏—Ç–∏—á–Ω–æ |

### –ü—Ä–∏–º–µ—Ä —Ä–∞—Å—á–µ—Ç–∞

**–ó–∞–ø—Ä–æ—Å:** "–†–µ–∫–ª–∞–º–∞ –∞–ª–∫–æ–≥–æ–ª—è –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"

**–î–æ–∫—É–º–µ–Ω—Ç #1234:**
```python
# –ö–æ—Å–∏–Ω—É—Å–Ω—ã–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ –ø–æ –ø–æ–ª—è–º:
R_FAS = 0.92  # FAS_arguments –æ—á–µ–Ω—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω
R_violation = 0.75  # violation_summary —Å—Ä–µ–¥–Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω
R_ad = 0.88  # addescription –æ—á–µ–Ω—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω

# –†–∞—Å—á–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –±–∞–ª–ª–∞:
S = 1.0 * 0.92 + 0.8 * 0.75 + 0.6 * 0.88
S = 0.92 + 0.60 + 0.528
S = 2.048
```

**–î–æ–∫—É–º–µ–Ω—Ç #5678:**
```python
R_FAS = 0.85
R_violation = 0.90
R_ad = 0.70

S = 1.0 * 0.85 + 0.8 * 0.90 + 0.6 * 0.70
S = 0.85 + 0.72 + 0.42
S = 1.99
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –î–æ–∫—É–º–µ–Ω—Ç #1234 (S=2.048) –±—É–¥–µ—Ç –≤—ã—à–µ #5678 (S=1.99), –ø–æ—Ç–æ–º—É —á—Ç–æ —É –Ω–µ–≥–æ –≤—ã—à–µ –æ—Ü–µ–Ω–∫–∞ —Å–∞–º–æ–≥–æ –≤–∞–∂–Ω–æ–≥–æ –ø–æ–ª—è (FAS_arguments).

### –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω–æ–∫

**–ó–∞—á–µ–º:** –ß—Ç–æ–±—ã –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ –ø–æ–ª–µ —Å –Ω–∏–∑–∫–∏–º –≤–µ—Å–æ–º –Ω–µ –ø–µ—Ä–µ–±–∏–≤–∞–ª –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç –≤ –ø–æ–ª–µ —Å –≤—ã—Å–æ–∫–∏–º –≤–µ—Å–æ–º.

```python
def normalize_score(score: float, min_score: float = 0.0, max_score: float = 1.0) -> float:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1]
    """
    if score < min_score:
        return 0.0
    if score > max_score:
        return 1.0
    return (score - min_score) / (max_score - min_score)

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:
for field in ['FAS_arguments', 'violation_summary', 'addescription']:
    raw_score = cosine_similarity(query_emb, field_emb)
    normalized = normalize_score(raw_score, min_score=-1.0, max_score=1.0)
    # –¢–µ–ø–µ—Ä—å normalized –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1]
```

---

## üéöÔ∏è –°–ò–°–¢–ï–ú–ê –§–ò–õ–¨–¢–†–û–í

### 4 —Ç–∏–ø–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤

#### 1. –§–∏–ª—å—Ç—Ä "–ì–æ–¥ —Ä–µ—à–µ–Ω–∏—è –§–ê–°"

**–ü–æ–ª–µ –ë–î:** `document_date` (timestamp)

**–õ–æ–≥–∏–∫–∞:**
```python
def filter_by_year(documents: List[Dict], years: List[int]) -> List[Dict]:
    """
    –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø–æ –≥–æ–¥—É —Ä–µ—à–µ–Ω–∏—è
    
    Args:
        documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        years: –°–ø–∏—Å–æ–∫ –≥–æ–¥–æ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä [2023, 2024]
    
    Returns:
        –î–æ–∫—É–º–µ–Ω—Ç—ã, –≥–¥–µ document_date.year in years
    """
    if not years:
        return documents
    
    return [
        doc for doc in documents
        if doc['document_date'].year in years
    ]
```

**–ü—Ä–∏–º–µ—Ä:**
```python
# –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤—ã–±—Ä–∞–ª: 2023, 2024
filtered = filter_by_year(documents, [2023, 2024])
# –í–µ—Ä–Ω—É—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç—ã 2023 –∏ 2024 –≥–æ–¥–æ–≤
```

#### 2. –§–∏–ª—å—Ç—Ä "–†–µ–≥–∏–æ–Ω (–£–§–ê–°)"

**–ü–æ–ª–µ –ë–î:** `FASdivision` (text)

**–ó–Ω–∞—á–µ–Ω–∏—è:**
- "–ú–æ—Å–∫–æ–≤—Å–∫–æ–µ –£–§–ê–° –†–æ—Å—Å–∏–∏"
- "–£–§–ê–° –ø–æ –≥. –ú–æ—Å–∫–≤–µ"
- "–£–§–ê–° –ø–æ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥—É"
- –∏ —Ç.–¥.

**–õ–æ–≥–∏–∫–∞:**
```python
def filter_by_region(documents: List[Dict], regions: List[str]) -> List[Dict]:
    """
    –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø–æ —Ä–µ–≥–∏–æ–Ω—É –£–§–ê–°
    
    Args:
        documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        regions: –°–ø–∏—Å–æ–∫ —Ä–µ–≥–∏–æ–Ω–æ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä ["–ú–æ—Å–∫–≤–∞", "–°–ü–±"]
    
    Returns:
        –î–æ–∫—É–º–µ–Ω—Ç—ã, –≥–¥–µ FASdivision in regions
    """
    if not regions:
        return documents
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π —Ä–µ–≥–∏–æ–Ω–æ–≤
    normalized_regions = [normalize_region_name(r) for r in regions]
    
    return [
        doc for doc in documents
        if normalize_region_name(doc['FASdivision']) in normalized_regions
    ]

def normalize_region_name(region: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–∞
    
    "–ú–æ—Å–∫–æ–≤—Å–∫–æ–µ –£–§–ê–° –†–æ—Å—Å–∏–∏" -> "–ú–æ—Å–∫–≤–∞"
    "–£–§–ê–° –ø–æ –≥. –ú–æ—Å–∫–≤–µ" -> "–ú–æ—Å–∫–≤–∞"
    """
    mapping = {
        "–ú–æ—Å–∫–æ–≤—Å–∫–æ–µ –£–§–ê–° –†–æ—Å—Å–∏–∏": "–ú–æ—Å–∫–≤–∞",
        "–£–§–ê–° –ø–æ –≥. –ú–æ—Å–∫–≤–µ": "–ú–æ—Å–∫–≤–∞",
        "–£–§–ê–° –ø–æ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥—É": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥",
        # ... –¥–æ–±–∞–≤–∏—Ç—å –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–≥–∏–æ–Ω—ã
    }
    return mapping.get(region, region)
```

#### 3. –§–∏–ª—å—Ç—Ä "–û—Ç—Ä–∞—Å–ª—å –ª–∏—Ü–∞"

**–ü–æ–ª–µ –ë–î:** `defendant_industry` (text)

**–ó–Ω–∞—á–µ–Ω–∏—è:**
- "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —É—Å–ª—É–≥–∏"
- "–°—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ"
- "–†–æ–∑–Ω–∏—á–Ω–∞—è —Ç–æ—Ä–≥–æ–≤–ª—è"
- "–ú–µ–¥–∏—Ü–∏–Ω–∞"
- –∏ —Ç.–¥.

**–õ–æ–≥–∏–∫–∞:**
```python
def filter_by_industry(documents: List[Dict], industries: List[str]) -> List[Dict]:
    """
    –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø–æ –æ—Ç—Ä–∞—Å–ª–∏ –Ω–∞—Ä—É—à–∏—Ç–µ–ª—è
    
    Args:
        documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        industries: –°–ø–∏—Å–æ–∫ –æ—Ç—Ä–∞—Å–ª–µ–π
    
    Returns:
        –î–æ–∫—É–º–µ–Ω—Ç—ã, –≥–¥–µ defendant_industry in industries
    """
    if not industries:
        return documents
    
    return [
        doc for doc in documents
        if doc['defendant_industry'] in industries
    ]
```

#### 4. –§–∏–ª—å—Ç—Ä "–°—Ç–∞—Ç—å—è –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–∫—Ç–∞"

**–ü–æ–ª–µ –ë–î:** `legal_provisions` (text, –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç–∞—Ç–µ–π)

**–ó–Ω–∞—á–µ–Ω–∏—è:**
- "—Å—Ç. 5"
- "—Å—Ç. 24"
- "—Å—Ç. 28"
- "—Å—Ç. 5 —á. 7"
- –∏ —Ç.–¥.

**–õ–æ–≥–∏–∫–∞:**
```python
def filter_by_article(documents: List[Dict], articles: List[str]) -> List[Dict]:
    """
    –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø–æ —Å—Ç–∞—Ç—å–µ –∑–∞–∫–æ–Ω–∞
    
    Args:
        documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        articles: –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π, –Ω–∞–ø—Ä–∏–º–µ—Ä ["—Å—Ç. 5", "—Å—Ç. 24"]
    
    Returns:
        –î–æ–∫—É–º–µ–Ω—Ç—ã, –≥–¥–µ legal_provisions —Å–æ–¥–µ—Ä–∂–∏—Ç —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –∏–∑ —Å—Ç–∞—Ç–µ–π
    """
    if not articles:
        return documents
    
    return [
        doc for doc in documents
        if any(article in doc['legal_provisions'] for article in articles)
    ]
```

**–ü—Ä–∏–º–µ—Ä:**
```python
# –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤—ã–±—Ä–∞–ª: "—Å—Ç. 5", "—Å—Ç. 24"
filtered = filter_by_article(documents, ["—Å—Ç. 5", "—Å—Ç. 24"])

# –î–æ–∫—É–º–µ–Ω—Ç —Å legal_provisions = "—Å—Ç. 5, —Å—Ç. 7" ‚Üí –≤–∫–ª—é—á–µ–Ω (—Å–æ–¥–µ—Ä–∂–∏—Ç "—Å—Ç. 5")
# –î–æ–∫—É–º–µ–Ω—Ç —Å legal_provisions = "—Å—Ç. 24" ‚Üí –≤–∫–ª—é—á–µ–Ω (—Å–æ–¥–µ—Ä–∂–∏—Ç "—Å—Ç. 24")
# –î–æ–∫—É–º–µ–Ω—Ç —Å legal_provisions = "—Å—Ç. 28" ‚Üí –ù–ï –≤–∫–ª—é—á–µ–Ω (–Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–∏ "—Å—Ç. 5", –Ω–∏ "—Å—Ç. 24")
```

### –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤

**–ü—Ä–∞–≤–∏–ª–æ:** –ú–µ–∂–¥—É —Ñ–∏–ª—å—Ç—Ä–∞–º–∏ - AND (–ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ), –≤–Ω—É—Ç—Ä–∏ —Ñ–∏–ª—å—Ç—Ä–∞ - OR (–æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ)

**–ü—Ä–∏–º–µ—Ä:**
```
–ó–∞–ø—Ä–æ—Å: "–†–µ–∫–ª–∞–º–∞ –∞–ª–∫–æ–≥–æ–ª—è"
–§–∏–ª—å—Ç—Ä—ã:
  –ì–æ–¥ = [2023, 2024]
  –†–µ–≥–∏–æ–Ω = ["–ú–æ—Å–∫–≤–∞", "–°–ü–±"]
  –°—Ç–∞—Ç—å—è = ["—Å—Ç. 5"]

–õ–æ–≥–∏–∫–∞:
  (–ì–æ–¥ = 2023 OR –ì–æ–¥ = 2024)
  AND
  (–†–µ–≥–∏–æ–Ω = –ú–æ—Å–∫–≤–∞ OR –†–µ–≥–∏–æ–Ω = –°–ü–±)
  AND
  (–°—Ç–∞—Ç—å—è —Å–æ–¥–µ—Ä–∂–∏—Ç "—Å—Ç. 5")
```

**–ö–æ–¥:**
```python
def apply_all_filters(documents: List[Dict], filters: Dict) -> List[Dict]:
    """
    –ü—Ä–∏–º–µ–Ω–∏—Ç—å –≤—Å–µ —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ (AND –º–µ–∂–¥—É —Ñ–∏–ª—å—Ç—Ä–∞–º–∏)
    """
    result = documents
    
    # –§–∏–ª—å—Ç—Ä 1: –ì–æ–¥ (OR –≤–Ω—É—Ç—Ä–∏)
    if filters.get('year'):
        result = filter_by_year(result, filters['year'])
    
    # –§–∏–ª—å—Ç—Ä 2: –†–µ–≥–∏–æ–Ω (OR –≤–Ω—É—Ç—Ä–∏)
    if filters.get('region'):
        result = filter_by_region(result, filters['region'])
    
    # –§–∏–ª—å—Ç—Ä 3: –û—Ç—Ä–∞—Å–ª—å (OR –≤–Ω—É—Ç—Ä–∏)
    if filters.get('industry'):
        result = filter_by_industry(result, filters['industry'])
    
    # –§–∏–ª—å—Ç—Ä 4: –°—Ç–∞—Ç—å—è (OR –≤–Ω—É—Ç—Ä–∏)
    if filters.get('article'):
        result = filter_by_article(result, filters['article'])
    
    return result
```

---

## üì¶ –ü–†–ò–ú–ï–†–´ –ü–û–õ–ù–û–ì–û –ö–û–î–ê

### –ü–æ–ª–Ω—ã–π API endpoint

```python
# backend/api/search.py
from fastapi import APIRouter, Query
from typing import List, Optional
from pydantic import BaseModel

router = APIRouter()

class SearchRequest(BaseModel):
    query: str
    year: Optional[List[int]] = None
    region: Optional[List[str]] = None
    industry: Optional[List[str]] = None
    article: Optional[List[str]] = None

class SearchResult(BaseModel):
    document_url: str
    document_date: str
    FASdivision: str
    defendant_industry: str
    legal_provisions: str
    excerpt: str  # –ö—Ä–∞—Ç–∫–∏–π –æ—Ç—Ä—ã–≤–æ–∫
    final_score: float
    field_scores: dict

@router.post("/search", response_model=List[SearchResult])
async def search_documents(request: SearchRequest):
    """
    API endpoint –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    
    POST /api/search
    {
        "query": "–†–µ–∫–ª–∞–º–∞ –∞–ª–∫–æ–≥–æ–ª—è –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ",
        "year": [2023, 2024],
        "region": ["–ú–æ—Å–∫–≤–∞"],
        "article": ["—Å—Ç. 5"]
    }
    
    Returns:
        –°–ø–∏—Å–æ–∫ –∏–∑ –º–∞–∫—Å–∏–º—É–º 10 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    """
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã
    filters = {
        'year': request.year,
        'region': request.region,
        'industry': request.industry,
        'article': request.article
    }
    
    # –í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫
    results = search_service.search(
        query=request.query,
        filters=filters
    )
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç API
    return [
        SearchResult(
            document_url=doc['document_url'],
            document_date=doc['document_date'].isoformat(),
            FASdivision=doc['FASdivision'],
            defendant_industry=doc['defendant_industry'],
            legal_provisions=doc['legal_provisions'],
            excerpt=create_excerpt(doc, request.query),
            final_score=doc['final_score'],
            field_scores=doc['field_scores']
        )
        for doc in results
    ]

def create_excerpt(doc: dict, query: str, max_length: int = 200) -> str:
    """
    –°–æ–∑–¥–∞—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç—Ä—ã–≤–æ–∫ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    """
    # –í–∑—è—Ç—å —Å–∞–º–æ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–µ –ø–æ–ª–µ
    best_field = max(doc['field_scores'], key=doc['field_scores'].get)
    text = doc[best_field]
    
    # –û–±—Ä–µ–∑–∞—Ç—å –¥–æ max_length
    if len(text) > max_length:
        text = text[:max_length] + "..."
    
    return text
```

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º

```javascript
// frontend/src/api/search.js
export async function searchDocuments(query, filters) {
  const response = await fetch('/api/search', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      query: query,
      year: filters.year || null,
      region: filters.region || null,
      industry: filters.industry || null,
      article: filters.article || null,
    }),
  });
  
  if (!response.ok) {
    throw new Error('Search failed');
  }
  
  return await response.json();
}

// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
const results = await searchDocuments(
  "–†–µ–∫–ª–∞–º–∞ –∞–ª–∫–æ–≥–æ–ª—è –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ",
  {
    year: [2023, 2024],
    region: ["–ú–æ—Å–∫–≤–∞"],
    article: ["—Å—Ç. 5"]
  }
);

console.log(`–ù–∞–π–¥–µ–Ω–æ ${results.length} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤`);
results.forEach(doc => {
  console.log(`${doc.document_url}: ${doc.final_score.toFixed(3)}`);
});
```

---

## ‚úÖ –ß–ï–ö–õ–ò–°–¢ –†–ï–ê–õ–ò–ó–ê–¶–ò–ò

### –ö—Ä–∏—Ç–∏—á–Ω–æ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ):
- [ ] –£–¥–∞–ª–∏–ª–∏ sentence-transformers, —É—Å—Ç–∞–Ω–æ–≤–∏–ª–∏ google-generativeai
- [ ] –ü–æ–ª—É—á–∏–ª–∏ GOOGLE_API_KEY, –¥–æ–±–∞–≤–∏–ª–∏ –≤ .env
- [ ] –í–µ–∑–¥–µ –∑–∞–º–µ–Ω–∏–ª–∏ 384 ‚Üí 384
- [ ] –û–±–Ω–æ–≤–∏–ª–∏ —Å—Ö–µ–º—É –ë–î (vector(384))
- [ ] –ü–µ—Ä–µ—Å–æ–∑–¥–∞–ª–∏ –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ Google Gemini
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞–ª–∏ –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É (TOP-50 ‚Üí —Ñ–∏–ª—å—Ç—Ä—ã ‚Üí TOP-10)
- [ ] –î–æ–±–∞–≤–∏–ª–∏ –≤–∑–≤–µ—à–µ–Ω–Ω—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (1.0, 0.8, 0.6)
- [ ] –§–∏–ª—å—Ç—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –ü–û–°–õ–ï –ø–æ–∏—Å–∫–∞
- [ ] –í—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–æ–≤–Ω–æ 10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### –ñ–µ–ª–∞—Ç–µ–ª—å–Ω–æ:
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞–ª–∏ –≤—Å–µ 4 —Ñ–∏–ª—å—Ç—Ä–∞
- [ ] –î–æ–±–∞–≤–∏–ª–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –æ—Ü–µ–Ω–æ–∫
- [ ] –î–æ–±–∞–≤–∏–ª–∏ –∑–æ–ª–æ—Ç–æ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç (20 —Ç–µ—Å—Ç–æ–≤)
- [ ] –û–±–Ω–æ–≤–∏–ª–∏ UX-—Ç–µ–∫—Å—Ç—ã
- [ ] –î–æ–±–∞–≤–∏–ª–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
- [ ] –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–ª–∏ SQL-–∑–∞–ø—Ä–æ—Å—ã —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏
- [ ] –î–æ–±–∞–≤–∏–ª–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

### –£–¥–∞–ª–∏—Ç—å:
- [ ] –í–µ—Å—å –∫–æ–¥ —Å sentence-transformers
- [ ] Hybrid search (—Å–µ–º–∞–Ω—Ç–∏–∫–∞ + –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)
- [ ] –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –î–û –ø–æ–∏—Å–∫–∞
- [ ] –í—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 384

---

## üö® –ß–ê–°–¢–´–ï –û–®–ò–ë–ö–ò –ò –†–ï–®–ï–ù–ò–Ø

### –û—à–∏–±–∫–∞ 1: GOOGLE_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω
```
ValueError: GOOGLE_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env
```

**–†–µ—à–µ–Ω–∏–µ:**
1. –°–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª `.env` –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞
2. –î–æ–±–∞–≤–∏—Ç—å —Å—Ç—Ä–æ–∫—É: `GOOGLE_API_KEY=your_actual_key`
3. –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ load_dotenv() –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –¥–æ –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π

### –û—à–∏–±–∫–∞ 2: –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
```
AssertionError: –û–∂–∏–¥–∞–µ—Ç—Å—è 384, –ø–æ–ª—É—á–µ–Ω–æ 384
```

**–†–µ—à–µ–Ω–∏–µ:**
- –ù–∞–π—Ç–∏ –≤—Å–µ –º–µ—Å—Ç–∞ —Å `384` –∏ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ `384`
- –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (`embeddings.npy`)
- –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É –ë–î —Å vector(384)

### –û—à–∏–±–∫–∞ 3: –ü—É—Å—Ç–∞—è –≤—ã–¥–∞—á–∞ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤
```
–ù–∞–π–¥–µ–Ω–æ 0 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
```

**–†–µ—à–µ–Ω–∏–µ:**
- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ —Ñ–∏–ª—å—Ç—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –∫ TOP-50, –∞ –Ω–µ –∫–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏–∫—É OR –≤–Ω—É—Ç—Ä–∏ —Ñ–∏–ª—å—Ç—Ä–æ–≤
- –î–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ: —Å–∫–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Ñ–∏–ª—å—Ç—Ä–∞

### –û—à–∏–±–∫–∞ 4: –ú–µ–¥–ª–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫
```
–ó–∞–ø—Ä–æ—Å –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è > 5 —Å–µ–∫—É–Ω–¥
```

**–†–µ—à–µ–Ω–∏–µ:**
- –î–æ–±–∞–≤–∏—Ç—å –∏–Ω–¥–µ–∫—Å—ã –Ω–∞ vector columns:
  ```sql
  CREATE INDEX ON documents USING ivfflat (FAS_arguments_embedding vector_cosine_ops);
  ```
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∞—Ç—á–∏–Ω–≥ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
- –ö—ç—à–∏—Ä–æ–≤–∞—Ç—å —á–∞—Å—Ç—ã–µ –∑–∞–ø—Ä–æ—Å—ã

---

**–í–µ—Ä—Å–∏—è:** 2.1  
**–î–∞—Ç–∞:** 21 —è–Ω–≤–∞—Ä—è 2026  
**–°—Ç–∞—Ç—É—Å:** ‚úÖ –ì–æ—Ç–æ–≤–æ –∫ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏